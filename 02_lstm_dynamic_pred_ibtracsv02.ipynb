{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short-term prediction using LSTM and IBTrACS\n",
    "\n",
    "v02: Input tensors are multivariate sequence of observations, thus have a shape (n_samples, timestamp, n_features) = (n_samples, 4, 5).\n",
    "Uses Dataset and DataLoader.\n",
    "\n",
    "TODO: use GPU inside ShortTimeseriesDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "# import glob\n",
    "import os.path\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Arrays & Displays\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# # from matplotlib.colors import Normalize\n",
    "# # from matplotlib.colors import ListedColormap\n",
    "# # import matplotlib.cm as cm\n",
    "# import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable              # Convert arrays to tensors\n",
    "from torch.utils.data import Dataset, DataLoader # Create a Dataset class to combine with DataLoader (= mini batches selection)\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "# Data treatment\n",
    "# import dask as da\n",
    "# from dask.diagnostics import ProgressBar\n",
    "# import zarr\n",
    "# from scipy.interpolate import griddata\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom\n",
    "import dataUtils    as du\n",
    "import pytorchUtils as pu\n",
    "\n",
    "# Statistics\n",
    "from sklearn import linear_model, neighbors\n",
    "\n",
    "# Default parameters\n",
    "mpl.rcParams.update({'font.size': 18})\n",
    "mpl.rcParams['figure.figsize'] = (15, 10)\n",
    "mpl.rcParams['axes.facecolor'] = 'white'\n",
    "mpl.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "1 GPU(s) available\n"
     ]
    }
   ],
   "source": [
    "### Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))\n",
    "print('{} GPU(s) available'.format(torch.cuda.device_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMS\n",
    "PARAMS = {'seq_len':     4, # length of the input time series used to predict y(t)\n",
    "          'n_features':  5,     # nb of output features\n",
    "          \n",
    "          # Model parameters\n",
    "          'input_size':  5,     # nb of input features\n",
    "          'hidden_size': 2,     # nb of features in hidden state\n",
    "          'num_layers':  1,     # nb of stacked lstm layers\n",
    "    \n",
    "          'batch_size':  8,     \n",
    "          'n_epochs':    10,     # nb of epochs\n",
    "          'learn_rate':  0.001, # learning rate\n",
    "          \n",
    "          'save_figs':       False,\n",
    "          'feature_scaling': True,\n",
    "         }\n",
    "\n",
    "### PATHS\n",
    "PATHS  = {\n",
    "    # Data\n",
    "    'ibtracs_data': '/home/arthur/data/ibtracs/IBTrACS.NA.v04r00.nc', # '/home/arthur/data/ibtracs/IBTrACS.NA.v04r00.nc'\n",
    "    # Save\n",
    "    'lstm_path':    '/home/arthur/results/TCsLifeMonitFromObs/lstm_dynamic_pred_ibtracs/lstm.pth', \n",
    "}\n",
    "\n",
    "### VARIABLES TO CONSIDER\n",
    "'''Initial dataset has 147 variables, so we select only a subset of these'''\n",
    "# storm speed, time, dist2land, usa_r64, usa_r50\n",
    "params_of_interest = ['usa_lon', 'usa_lat', 'usa_wind', 'usa_r34', 'usa_rmw'] \n",
    "additional_info    = ['numobs', 'sid', 'basin', 'name', 'usa_agency', 'iso_time', 'nature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPEN DATASET\n",
    "ds_ibt_raw = xr.open_dataset(PATHS['ibtracs_data'])\n",
    "ds_ibt     = ds_ibt_raw[params_of_interest + additional_info]\n",
    "# ds_ibt_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILTERS\n",
    "# By year\n",
    "start_date = np.datetime64('2000-01-01')\n",
    "fsi        = np.nanargmin(np.abs(ds_ibt['time'][:, 0] - start_date)) # First storm index\n",
    "ds_ibt     = ds_ibt.isel(storm=slice(fsi, -1))\n",
    "# By latitude\n",
    "ds_ibt     = ds_ibt.where(np.abs(ds_ibt['lat']) <= 30)\n",
    "# By removing empty Rmax time series\n",
    "ds_ibt     = ds_ibt.where(ds_ibt['usa_rmw'].notnull().sum(axis=1) > 5)\n",
    "# By removing empty R34 time series\n",
    "ds_ibt     = ds_ibt.where(ds_ibt['usa_r34'].notnull().sum(axis=1) > 5) \n",
    "# By agency\n",
    "if os.path.basename(PATHS['ibtracs_data']) == 'IBTrACS.NA.v04r00.nc': ds_ibt     = ds_ibt.where(ds_ibt['usa_agency'] == b'hurdat_atl')\n",
    "\n",
    "# ==> Drop NaNs\n",
    "ds_ibt     = ds_ibt.dropna(dim='storm', how='all') # Much quicker to drop NaNs only at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR_variables = ['usa_lon', 'usa_lat', 'usa_r34']\n",
    "# ds_ibt_LR    = ds_ibt[LR_variables]\n",
    "# ds_ibt_LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 188 storms;   Test set: 48 storms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 188/188 [00:03<00:00, 58.63it/s]\n",
      "100%|███████████████████████████████████████████| 48/48 [00:00<00:00, 52.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of predictors matrix X_train:  (2432, 20)\n",
      "Shape of targets matrix y_train:  (2432, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### CREATE DATASET (Train and test)\n",
    "# FIRST WE AVERAGE OVER EVERY QUADRANT\n",
    "# Pre-processing\n",
    "MU    = {}\n",
    "SIG   = {}\n",
    "SCALE = {'usa_wind': 3,\n",
    "         'usa_rmw':  3, \n",
    "         'usa_r34':  2,\n",
    "         'usa_lon':  0.5,\n",
    "         'usa_lat':  1,\n",
    "}\n",
    "if PARAMS['feature_scaling']:\n",
    "    for param in params_of_interest:\n",
    "        MU[param]     = float(ds_ibt[param].mean(skipna=True))\n",
    "        SIG[param]    = float(ds_ibt[param].std(skipna=True))\n",
    "        ds_ibt[param] = SCALE[param] * ((ds_ibt[param] - MU[param]) / SIG[param])\n",
    "\n",
    "# Separate train and test set\n",
    "sep = int(0.8 * len(ds_ibt['storm'])) # 80% train, 20% test\n",
    "ds_train, ds_test = ds_ibt.isel(storm=slice(None, sep)), ds_ibt.isel(storm=slice(sep, None))\n",
    "print('Train set: %i storms;  '%len(ds_train['storm']), 'Test set: %i storms'%len(ds_test['storm']))\n",
    "\n",
    "# Create Dataset\n",
    "X_train, y_train = du.create_dataset(ds_train, params_of_interest, PARAMS)\n",
    "X_test, y_test   = du.create_dataset(ds_test,  params_of_interest, PARAMS)\n",
    "print('Shape of predictors matrix X_train: ', np.asarray(X_train).shape)\n",
    "print('Shape of targets matrix y_train: ', np.asarray(y_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: torch.Size([8, 4, 5])\n",
      "Target shape: torch.Size([8, 5])\n"
     ]
    }
   ],
   "source": [
    "# Open Datasets\n",
    "train_dataset = pu.ShortTimeseriesDataset(X_train, \n",
    "                                          y_train,\n",
    "                                          PARAMS['n_features'],\n",
    "                                          PARAMS['seq_len']\n",
    "                                         )\n",
    "test_dataset  = pu.ShortTimeseriesDataset(X_test, \n",
    "                                          y_test,\n",
    "                                          PARAMS['n_features'],\n",
    "                                          PARAMS['seq_len']\n",
    "                                         )\n",
    "\n",
    "# DataLoader\n",
    "torch.manual_seed(99)\n",
    "train_loader = DataLoader(train_dataset, batch_size=PARAMS['batch_size'], shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=PARAMS['batch_size'], shuffle=False)\n",
    "\n",
    "X, y = next(iter(train_loader)) # Check\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM1(\n",
      "  (lstm): LSTM(5, 2, batch_first=True)\n",
      "  (fc_1): Linear(in_features=2, out_features=128, bias=True)\n",
      "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Declare model\n",
    "# Normally, LSTM1 handles batch_size\n",
    "model = pu.LSTM1(num_classes=PARAMS['n_features'], \n",
    "                 input_size=PARAMS['input_size'],\n",
    "                 hidden_size=PARAMS['hidden_size'],\n",
    "                 num_layers=PARAMS['num_layers'],\n",
    "                 seq_len=PARAMS['seq_len'],             \n",
    ")\n",
    "print(model)\n",
    "\n",
    "# Loss function, optimizer\n",
    "loss_function = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer     = torch.optim.Adam(model.parameters(), lr=PARAMS['learn_rate']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained test\n",
      "--------\n",
      "Test loss: 4.6562081020453885\n",
      "\n",
      "Epoch 0\n",
      "---------\n",
      "Train loss: 3.4222859210873904\n",
      "Test loss: 2.6662339992906854\n",
      "\n",
      "Epoch 1\n",
      "---------\n",
      "Train loss: 2.4285676579334234\n",
      "Test loss: 2.1775875132659386\n",
      "\n",
      "Epoch 2\n",
      "---------\n",
      "Train loss: 1.8200904832858789\n",
      "Test loss: 1.6099481654578243\n",
      "\n",
      "Epoch 3\n",
      "---------\n",
      "Train loss: 1.3791104797273874\n",
      "Test loss: 1.3881917876758794\n",
      "\n",
      "Epoch 4\n",
      "---------\n",
      "Train loss: 1.229969138867761\n",
      "Test loss: 1.2994786134396477\n",
      "\n",
      "Epoch 5\n",
      "---------\n",
      "Train loss: 1.1605768213538747\n",
      "Test loss: 1.2725745112388984\n",
      "\n",
      "Epoch 6\n",
      "---------\n",
      "Train loss: 1.1092506685343229\n",
      "Test loss: 1.2366023462736744\n",
      "\n",
      "Epoch 7\n",
      "---------\n",
      "Train loss: 1.065027684757584\n",
      "Test loss: 1.1757734428192008\n",
      "\n",
      "Epoch 8\n",
      "---------\n",
      "Train loss: 1.0262919734967382\n",
      "Test loss: 1.1189834751274395\n",
      "\n",
      "Epoch 9\n",
      "---------\n",
      "Train loss: 0.9758337972391593\n",
      "Test loss: 1.0353985752525001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss  = 0\n",
    "    model.train()\n",
    "\n",
    "    for X, y in data_loader:\n",
    "        output = model(X)\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Train loss: {avg_loss}\")\n",
    "\n",
    "def test_model(data_loader, model, loss_function):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss  = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            output = model(X)\n",
    "            total_loss += loss_function(output, y).item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Test loss: {avg_loss}\")\n",
    "\n",
    "def predict(data_loader, model):\n",
    "\n",
    "    output = torch.tensor([])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in data_loader:\n",
    "            y_star = model(X)\n",
    "            output = torch.cat((output, y_star), 0)\n",
    "\n",
    "    return output\n",
    "\n",
    "print(\"Untrained test\\n--------\")\n",
    "test_model(test_loader, model, loss_function)\n",
    "print()\n",
    "\n",
    "for ix_epoch in range(PARAMS['n_epochs']):\n",
    "    print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "    train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "    test_model(test_loader, model, loss_function)\n",
    "    print()\n",
    "    \n",
    "# # Save model\n",
    "# torch.save(lstm1.state_dict(), PATHS['lstm_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference using a new dataloader (validation_dataloader for instance, creatded using ds.isel(slice(-3, -2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 5, got 20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2744/431379109.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tensors_final\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# caluclate the gradient, manually setting to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scripts/TCsLifeMonitFromObs/pytorchUtils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mc_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# internal state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Propagate input through LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# lstm with input, hidden, and internal state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mhn\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mhn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reshaping the data for Dense layer next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/paramProfiles/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/paramProfiles/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/anaconda3/envs/paramProfiles/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    630\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                            ):\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[1;32m    634\u001b[0m                                'Expected hidden[0] size {}, got {}')\n",
      "\u001b[0;32m~/anaconda3/envs/paramProfiles/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    203\u001b[0m                     expected_input_dim, input.dim()))\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    206\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[1;32m    207\u001b[0m                     self.input_size, input.size(-1)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 5, got 20"
     ]
    }
   ],
   "source": [
    "# # Move model and Train set to GPU\n",
    "# lstm1.to(device)\n",
    "# X_train_tensors_final = X_train_tensors_final.to(device)\n",
    "# y_train_tensors       = y_train_tensors.to(device)\n",
    "# # NB LSTM() model also moves tensors to device inside the class (for hidden tensors)\n",
    "\n",
    "# # Training\n",
    "# for epoch in range(2000):\n",
    "#     outputs = lstm1.forward(X_train_tensors_final) # forward pass\n",
    "#     optimizer.zero_grad() # caluclate the gradient, manually setting to 0\n",
    "\n",
    "#     # obtain the loss function\n",
    "#     loss = criterion(outputs, y_train_tensors)\n",
    "\n",
    "#     loss.backward() # calculates the loss of the loss function\n",
    "#     optimizer.step() #improve from loss, i.e backprop\n",
    "    \n",
    "#     if epoch % 100 == 0:\n",
    "#         print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare inference data\n",
    "# X_im, y_im         = du.create_dataset(ds_test.isel(storm=slice(-30, -20)),  params_of_interest, PARAMS)\n",
    "# y_im               = np.array(y_im)\n",
    "# X_im_tensors       = Variable(torch.Tensor(X_im))\n",
    "# X_im_tensors_final = torch.reshape(X_im_tensors, (X_im_tensors.shape[0], 1, X_im_tensors.shape[1]))\n",
    "\n",
    "# # Inference\n",
    "# lstm1.eval()\n",
    "# lstm1.cpu()\n",
    "# with torch.no_grad():\n",
    "#     pred = lstm1(X_im_tensors_final)\n",
    "\n",
    "# # Plot\n",
    "# for i, param in enumerate(params_of_interest):\n",
    "#     feature = pred[:, i].cpu().detach().numpy()\n",
    "#     feature = du.inverse_scale_normalize(feature, MU, SIG, SCALE, param)\n",
    "#     target  = y_im[:, i]\n",
    "#     target  = du.inverse_scale_normalize(target,  MU, SIG, SCALE, param)\n",
    "#     plt.title(param, weight='bold')\n",
    "#     plt.plot(feature, label='LSTM', color='tab:pink')\n",
    "#     plt.plot(target,  label='Data', linewidth=3, color='tab:blue')\n",
    "#     plt.legend(loc='upper left');plt.grid()\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paramProfiles",
   "language": "python",
   "name": "paramprofiles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
