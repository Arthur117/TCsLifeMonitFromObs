{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSV files for AnDA - version 2\n",
    "\n",
    "Objectives: \n",
    "\n",
    "- Open IBTrACS, filter it and interpolate data such that time step is every hour. \n",
    "\n",
    "- Then, for each storm, find all collocated SAR, RAD and SCT data, and add it to the timestep.\n",
    "\n",
    "CAVEAT: I need to pay attention on how I interpolate longitudes. Also, I didn't select latitude < 30° so far.\n",
    "\n",
    "TODO: Add a column for time. interpolate to 1h timesteps. Add observations to closest timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import glob\n",
    "import os.path\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "\n",
    "# Arrays & Displays\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "\n",
    "# Data treatment\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom\n",
    "import functions as f\n",
    "import dataUtils as du\n",
    "\n",
    "# Default parameters\n",
    "mpl.rcParams.update({'font.size': 18})\n",
    "mpl.rcParams['figure.figsize'] = (15, 10)\n",
    "mpl.rcParams['axes.facecolor'] = 'white'\n",
    "mpl.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS  = {\n",
    "    'obs_dir':     '/home/arthur/results/ck22model/Radiometer_applications/02_inferences/obs_nc_files/',\n",
    "    'ibt_pth':     '/home/arthur/data/ibtracs/IBTrACS.ALL.v04r00.nc',\n",
    "    'sav_csv_dir': '/home/arthur/scripts/TCsLifeMonitFromObs/AnDA/AnDA_data/IBTrACS_V9/',\n",
    "    'sav_fig_dir': '/home/arthur/results/TCsLifeMonitFromObs/00_create_csv_AnDA_v02/'\n",
    "}\n",
    "\n",
    "PARAMS = {\n",
    "    'csv_vars2keep':  ['usa_wind', 'usa_rmw', 'rmx_ck22', 'mean_r34', 'fcor'],\n",
    "    'csv_var_names':  ['Vmax',     'Rmax',    'rmx_ck22', 'R34',      'fcor'],\n",
    "    'ibt_vars2keep':  ['usa_lon', 'usa_lat', 'usa_wind', 'usa_r34', 'usa_rmw', 'storm_speed', 'storm_dir', 'name', 'sid'],\n",
    "    'ibt_info2keep':  ['sid', 'basin', 'name', 'usa_atcf_id', 'usa_agency', 'iso_time', 'usa_status'],\n",
    "    'LMI_min':        33,  # If a storm doesn't reach LMI_min m/s along its life cycle, it is discarded\n",
    "    'min_valid_rmxs': 24,  # If a storm has less than min_valid_rmxs timesteps with valid values of usa_rmw, it is discarded\n",
    "    'min_seq_len':    24,  # If a storm has less than 24 valid timesteps IN A ROW for usa_wind, usa_rmw, and mean_r34 after selecting 17.5 ms < t < LMI, it is discarded\n",
    "    'save_csv_files': True, \n",
    "    'save_figs':      False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPEN IBTrACS\n",
    "ds_ibt_raw = xr.open_dataset(PATHS['ibt_pth'])\n",
    "ds_ibt     = ds_ibt_raw[PARAMS['ibt_vars2keep']]\n",
    "ds_ibt[PARAMS['ibt_info2keep']] = ds_ibt_raw[PARAMS['ibt_info2keep']].astype(str, copy=False) # Convert byte arrays to strings\n",
    "\n",
    "start_date = np.datetime64('2000-01-01')\n",
    "fsi        = np.nanargmin(np.abs(ds_ibt['time'][:, 0] - start_date)) # First storm index\n",
    "ds_ibt     = ds_ibt.isel(storm=slice(fsi, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONVERSIONS\n",
    "# Convert to m/s and km units\n",
    "for speed in ['usa_wind', 'storm_speed']:\n",
    "    ds_ibt[speed]  *= 0.5144\n",
    "    ds_ibt[speed].attrs['units'] = 'm/s (original units from IBTrACS file: kts)'\n",
    "for radius in ['usa_r34', 'usa_rmw']:\n",
    "    ds_ibt[radius] *= 1.852\n",
    "    ds_ibt[radius].attrs['units'] = 'km (original units from IBTrACS file: n. mi)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IBTrACS interpolation\n",
    "# How to handle angles before interpolation\n",
    "def shortest_angle(angle_diff):\n",
    "    '''Given an angle_diff end - start between 2 angles, returns the shortest angle between 0 and 360 to go from start to end.'''\n",
    "    return ((angle_diff) + 180) % 360 - 180\n",
    "\n",
    "def rearange(da):\n",
    "    '''Given a dataArray of angles, reorganizes the angle values so that there is no jump.\n",
    "    For instance, given a sequence of angles [358, 359, 0, 1, 2], reorganizes that sequence into [358, 359, 360, 361, 362].\n",
    "    So that it can further be interpolated correctly: 1st apply this function. 2nd interpolate. 3rd apply modulo 360 to be consistent with the rest.\n",
    "    NB: John has a solution based on decomposing the angles on cosine and sine, then interpolating, anf finally taking the arctan2 of the ratio.\n",
    "    NB2: We could also detect the basin in which the storm is, and select an interval (e.g [-180, 180]) that works for each specific basin.\n",
    "    '''              \n",
    "    dav     = da.dropna(dim='date_time')\n",
    "    dav[1:] = shortest_angle(dav.diff(dim='date_time')) # Compute the discrete shortest angle difference between all angles 2 by 2 but let the first valid value intact\n",
    "    da[da.notnull()] = dav.cumsum(dim='date_time')      # Cumulative sum: starts from the 1st number then add all the differences previously computed step by step\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ab2710cb964fa281df857a55d1d1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples after removing LMI < 33 m/s and nb of valid values for Rmax > 24:  866\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02e946a55a44aaca251201bf9f0938b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h17ms successfully computed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e12266397f04518a9110a0764789190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/866 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples after selecting 17.5 m/s < t < LMI and keeping sequences that have more than 24 valid timesteps: 432\n"
     ]
    }
   ],
   "source": [
    "### 1) LMI >= 33 m/s\n",
    "ds_ibt['LMI_is_above_33ms'] = xr.DataArray(np.zeros_like(ds_ibt.storm), dims={'storm': ds_ibt.storm})\n",
    "msk = ds_ibt.usa_wind.max(dim='date_time', skipna=True) < PARAMS['LMI_min'] \n",
    "ds_ibt.LMI_is_above_33ms[~msk] = 1\n",
    "ds_ibt = ds_ibt.where(ds_ibt.LMI_is_above_33ms == 1, drop=True)\n",
    "\n",
    "### 2) At least 24 valid Rmax values\n",
    "for s in tqdm(range(len(ds_ibt['storm']))):\n",
    "    da = ds_ibt.isel(storm=s)['usa_rmw']\n",
    "    if np.count_nonzero(~np.isnan(da)) < PARAMS['min_valid_rmxs']: # Count the number of valid usa_rmw values and filter it out if less than 24 values\n",
    "        ds_ibt.isel(storm=s)['usa_rmw'] *= np.nan \n",
    "ds_ibt = ds_ibt.dropna(dim='storm', how='all', subset=['usa_rmw']) \n",
    "\n",
    "print(\"Total samples after removing LMI < {} m/s and nb of valid values for Rmax > {}: \".format(PARAMS['LMI_min'], PARAMS['min_valid_rmxs']), len(ds_ibt['storm']))\n",
    "\n",
    "### 3) Add nb_of_hours since 17.5 m/s have been achieved\n",
    "ds_ibt['i17ms'] = xr.DataArray(np.zeros_like(ds_ibt.storm), dims={'storm': ds_ibt.storm})\n",
    "ds_ibt['t17ms'] = xr.DataArray(np.tile(None, ds_ibt.storm.shape), dims={'storm': ds_ibt.storm}).astype(\"datetime64[ns]\")\n",
    "ds_ibt['h17ms'] = xr.DataArray(np.zeros_like(ds_ibt.usa_lon),     dims={'storm': ds_ibt.storm, 'date_time': ds_ibt.date_time})\n",
    "for s in tqdm(range(len(ds_ibt.storm))):\n",
    "    ds  = ds_ibt.isel(storm=s)\n",
    "    i17 = int(ds.date_time.where(ds.usa_wind > 17.5, drop=True)[0])\n",
    "    ds_ibt['i17ms'][s] = i17\n",
    "    ds_ibt['t17ms'][s] = ds.time[i17] # CAVEAT: If a time serie starts with usa_wind > 17.5 m/s, t17 will still be ds.time[0]\n",
    "    ds_ibt['h17ms'][s] = (ds.time - ds.time[i17]).astype(float) / (1e9) / 3600\n",
    "    ds_ibt['h17ms'][s][np.isnat(ds.time)] = np.nan\n",
    "    \n",
    "print('h17ms successfully computed.')\n",
    "\n",
    "### 4) Crop dataset between 17.5 ms and LMI\n",
    "ds_ibt['iLMI'] = ds_ibt.usa_wind.argmax(dim='date_time')\n",
    "for s in tqdm(range(len(ds_ibt.storm))):\n",
    "    ds = ds_ibt.isel(storm=s)\n",
    "    ds_ibt['usa_wind'][s, :int(ds.i17ms)] = np.nan\n",
    "    ds_ibt['usa_wind'][s, int(ds.iLMI):]  = np.nan\n",
    "    if np.count_nonzero(ds_ibt['usa_wind'][s].notnull()) < PARAMS['min_seq_len']:\n",
    "        ds_ibt['usa_wind'][s] = np.nan\n",
    "ds_ibt = ds_ibt.dropna(dim='storm', how='all', subset=['usa_wind']) \n",
    "\n",
    "print('Total samples after selecting 17.5 m/s < t < LMI and keeping sequences that have more than {} valid timesteps:'.format(PARAMS['min_seq_len']), len(ds_ibt['storm']))\n",
    "\n",
    "### 5) Add R34 and Coriolis parameter\n",
    "ds_ibt['mean_r34'] = ds_ibt.usa_r34.mean(dim='quadrant', skipna=True)\n",
    "ds_ibt['fcor']     = f.coriolis(np.abs(ds_ibt['usa_lat']))\n",
    "\n",
    "### 6) Attributes\n",
    "ds_ibt.LMI_is_above_33ms.attrs['description'] = 'A simple boolean (0 or 1) to say whether storm LMI is above 33 m/s (1) or not (0)'\n",
    "ds_ibt.i17ms.attrs['description'] = 'Index of the date_time dimension when storm has reached 17.5 m/s'\n",
    "ds_ibt.t17ms.attrs['description'] = 'UTC time of when storm has reached 17.5 m/s'\n",
    "ds_ibt.h17ms.attrs['description'] = 'Time difference (in hours) between current time step and t17ms: t_current - t17ms'\n",
    "ds_ibt.iLMI.attrs['description']  = 'Index of the date_time dimension when storm has reached LMI'\n",
    "ds_ibt.fcor.attrs['description']  = 'Average of the quadrant values of R34 in the IBTrACS, eventually skipping NaNs'\n",
    "ds_ibt.fcor.attrs['description']  = 'Coriolis parameter computed at the absolute value of the latitude, hence always positive'\n",
    "\n",
    "### 7) Assign Rmax CK22\n",
    "ds_ibt['rmx_ck22'] = f.get_rmax_ck22(ds_ibt['usa_wind'], ds_ibt['mean_r34'], ds_ibt['fcor'], 0.608, -0.00767, -0.00183) / 1000 # Coefficients of the paper\n",
    "\n",
    "# ### 4) FLIP TCs of Southern hemisphere\n",
    "# ### Take the symmetric with respect to Equator\n",
    "# ### So storm_dir = PI - storm_dir\n",
    "# ds_ibt.where(ds_ibt['lat'] < 0)['storm_dir'] = 180 - ds_ibt['storm_dir']\n",
    "# # Then project to (u, v) ref.\n",
    "# ds_ibt['u_trans'] = ds_ibt['storm_speed'] * np.sin(np.deg2rad(ds_ibt['storm_dir']))\n",
    "# ds_ibt['v_trans'] = ds_ibt['storm_speed'] * np.cos(np.deg2rad(ds_ibt['storm_dir']))\n",
    "# # plt.hist(np.array(ds_ibt['u_trans']).flatten()) # Check gaussianity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_ibt = ds_ibt.where(ds_ibt.name == 'SOUDELOR', drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_sar_or_not(path):\n",
    "    'Returns 1 if SAR data, 0 if not'\n",
    "    if path.split('/')[5] in ['radiometer', 'KNMI']:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30322eaeed44c4db2d0d186d81c107e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAVEAT: Storm 19 does not have 3-hrs timesteps. Storm discarded\n",
      "Total nb. of files: 285\n"
     ]
    }
   ],
   "source": [
    "nb_files = 0\n",
    "\n",
    "### CREATE CSV FILES\n",
    "for s in tqdm(range(len(ds_ibt.storm))):\n",
    "    ds  = ds_ibt.isel(storm=s)\n",
    "    I_MIN = 0\n",
    "    I_MAX = 359\n",
    "    for param in PARAMS['csv_vars2keep']:\n",
    "        i_min = int(ds.date_time[~np.isnan(ds[param])].min())\n",
    "        i_max = int(ds.date_time[~np.isnan(ds[param])].max())\n",
    "        if i_min > I_MIN:\n",
    "            I_MIN = i_min\n",
    "        if i_max < I_MAX:\n",
    "            I_MAX = i_max\n",
    "    ds = ds.isel(date_time=slice(I_MIN, I_MAX))\n",
    "    \n",
    "    ### CHECK AND FILTER\n",
    "    discard = False\n",
    "    # At least 24 valid values in a row\n",
    "    for param in ['usa_wind', 'usa_rmw', 'mean_r34', 'fcor']:\n",
    "        if np.count_nonzero(np.isnan(ds[param])) > 0:\n",
    "            val_len = np.max(np.diff(np.concatenate(([0], np.where(np.isnan(ds[param]))[0]), axis=0)))\n",
    "            if val_len < PARAMS['min_seq_len']:\n",
    "                discard = True\n",
    "    if len(ds.date_time) < PARAMS['min_seq_len']:\n",
    "        discard = True\n",
    "    # Every timestep must be a 3-hr timestep\n",
    "    if ds.h17ms.diff(dim='date_time').max(skipna=True) !=3 and not discard:\n",
    "        print('CAVEAT: Storm {} does not have 3-hrs timesteps. Storm discarded'.format(s))\n",
    "        discard = True\n",
    "    \n",
    "    # If there still is a nan after all this discard the sample\n",
    "    ds2keep = ds[PARAMS['csv_vars2keep']]\n",
    "    if np.count_nonzero(np.isnan(ds2keep.to_array())) > 0:\n",
    "        discard = True\n",
    "        \n",
    "    \n",
    "    if not discard:\n",
    "        nb_files += 1\n",
    "        \n",
    "        # Add observations\n",
    "        # Initialize empty arrays of observations and store observations in ds\n",
    "        # Observations will be added later to ds2keep\n",
    "        for p in ['vmx_obs', 'rmx_obs', 'r34_obs', 'vmx_sar', 'rmx_sar', 'r34_sar']:\n",
    "            ds[p] = xr.DataArray(np.full_like(ds.usa_wind, fill_value=np.nan), dims={'date_time': ds_ibt.time})\n",
    "        TCf = '{}_{}'.format(np.unique(ds.usa_atcf_id)[0].lower(), ds.name.data)\n",
    "        # Open observations if exist\n",
    "        if os.path.exists(PATHS['obs_dir'] + TCf):\n",
    "            ds_obs = xr.open_dataset(PATHS['obs_dir'] + TCf)\n",
    "            ds_obs['is_sar'] = xr.DataArray(np.array([is_sar_or_not(str(p.data)) for p in ds_obs.pth]), coords={'time': ds_obs.time}) # Separate data: 1 if SAR, 0 if not\n",
    "            for is_sar, dss in ds_obs.groupby(ds_obs.is_sar): \n",
    "                # Iterate over ds_obs, eventually fill observations\n",
    "                for i_o, t in enumerate(dss.time):\n",
    "                    t_dffs         = (t - ds.time) / np.timedelta64(1, 'h')\n",
    "                    idx_min, t_dff = np.nanargmin(abs(t_dffs)), np.nanmin(abs(t_dffs))\n",
    "                    if t_dff < 1.5:\n",
    "                        for p in ['vmx', 'rmx', 'r34']:\n",
    "                            if is_sar == 0: # Radiometer, scatterometer\n",
    "                                ds['{}_obs'.format(p)][idx_min] = dss.isel(time=i_o)[p]\n",
    "                            if is_sar == 1: # SAR\n",
    "                                ds['{}_sar'.format(p)][idx_min] = dss.isel(time=i_o)[p]\n",
    "        \n",
    "        # SAVE CSV\n",
    "        if PARAMS['save_csv_files']:\n",
    "            df      = pd.DataFrame(\n",
    "                data=ds2keep.to_array().transpose(),\n",
    "                columns=PARAMS['csv_var_names']\n",
    "            )\n",
    "            for p in ['vmx_obs', 'rmx_obs', 'r34_obs', 'vmx_sar', 'rmx_sar', 'r34_sar']:\n",
    "                df[p] = np.array(ds[p])\n",
    "            df.index = pd.to_datetime(ds2keep.time)\n",
    "            df.index.name = 'Time'\n",
    "            df.to_csv(PATHS['sav_csv_dir'] + '{}_{}'.format(np.unique(ds.usa_atcf_id)[0].lower(), ds.name.data))\n",
    "        \n",
    "        # PLOT\n",
    "        if PARAMS['save_figs']:\n",
    "            fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(20, 20))\n",
    "            for param, ax in zip(PARAMS['csv_vars2keep'], fig.axes):\n",
    "                ax.set_title(param, weight='bold')\n",
    "                ax.plot(ds.time, ds[param])\n",
    "                ax.grid()\n",
    "                ax.xaxis.set_major_formatter(mpl.dates.DateFormatter(\"%m-%d\"))\n",
    "            plt.savefig(PATHS['sav_fig_dir'] + TCf, bbox_inches = 'tight');plt.clf()\n",
    "            \n",
    "print('Total nb. of files: {}'.format(nb_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds_ibt.isel(storm=s)\n",
    "# ds = ds.isel(date_time=slice(I_MIN, I_MAX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in ['vmx_obs', 'rmx_obs', 'r34_obs', 'vmx_sar', 'rmx_sar', 'r34_sar']:\n",
    "#     ds[p] = xr.DataArray(np.full_like(ds.usa_wind, fill_value=np.nan), dims={'date_time': ds_ibt.time})\n",
    "# TCf = '{}_{}'.format(np.unique(ds.usa_atcf_id)[0].lower(), ds.name.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(PATHS['obs_dir'] + TCf):\n",
    "#     ds_obs = xr.open_dataset(PATHS['obs_dir'] + TCf)\n",
    "#     ds_obs['is_sar'] = xr.DataArray(np.array([is_sar_or_not(str(p.data)) for p in ds_obs.pth]), coords={'time': ds_obs.time}) # Separate data: 1 if SAR, 0 if not\n",
    "#     # ds_obs['is_sar'] = np.array([is_sar_or_not(str(p.data)) for p in ds_obs.pth]) # Separate data: 1 if SAR, 0 if not\n",
    "#     for is_sar, dss in ds_obs.groupby(ds_obs.is_sar): \n",
    "#         # Iterate over ds_obs, eventually fill observations\n",
    "#         for i_o, t in enumerate(dss.time):\n",
    "#             t_dffs         = (t - ds.time) / np.timedelta64(1, 'h')\n",
    "#             idx_min, t_dff = np.nanargmin(abs(t_dffs)), np.nanmin(abs(t_dffs))\n",
    "#             if t_dff < 1.5:\n",
    "#                 for p in ['vmx', 'rmx', 'r34']:\n",
    "#                     if is_sar == 0: # Radiometer, scatterometer\n",
    "#                         ds['{}_obs'.format(p)][idx_min] = dss.isel(time=i_o)[p]\n",
    "#                     if is_sar == 1: # SAR\n",
    "#                         ds['{}_sar'.format(p)][idx_min] = dss.isel(time=i_o)[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'rmx_sar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10933/954873561.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmx_sar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/paramProfiles/lib/python3.8/site-packages/xarray/core/common.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;34m\"{!r} object has no attribute {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'rmx_sar'"
     ]
    }
   ],
   "source": [
    "ds.rmx_sar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_obs.rmx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.rmx_sar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_obs.is_sar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in ds_obs.groupby(ds_obs.is_sar):\n",
    "    print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paramProfiles",
   "language": "python",
   "name": "paramprofiles"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
